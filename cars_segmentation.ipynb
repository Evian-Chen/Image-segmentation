{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaxoqFkMAVhJ",
        "outputId": "5b1d3dfa-22d6-460a-e738-43a3f94deed5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Mount to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khz6I7uQtyx_",
        "outputId": "8b5edcae-8c18-44ec-a500-a170a0b9467b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Cars\n"
          ]
        }
      ],
      "source": [
        "%cd gdrive/MyDrive/Cars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "s3gL3LFHAed8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OQospNIGB9CM"
      },
      "outputs": [],
      "source": [
        "# Build for UNet\n",
        "class DoubleConv(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super().__init__()\n",
        "\n",
        "    # twice conv2d\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),  # normalize every feature\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rZSFGD-sCHtV"
      },
      "outputs": [],
      "source": [
        "# Build UNet from scrach\n",
        "class UNet(nn.Module):\n",
        "  def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):  # in_channels is RGB, out_channels is i or 0(pixel level)\n",
        "    super().__init__()\n",
        "\n",
        "    # module list\n",
        "    self.downs = nn.ModuleList()\n",
        "    self.ups = nn.ModuleList()\n",
        "\n",
        "    # in_channels 3 to 64\n",
        "    # down-sampling\n",
        "    for feature in features:\n",
        "      self.downs.append(DoubleConv(in_channels, feature))\n",
        "      in_channels = feature\n",
        "\n",
        "    # 512(the last feature) to 1024\n",
        "    self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
        "\n",
        "    # up-sampling\n",
        "    for feature in reversed(features):\n",
        "      self.ups.append(nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2))\n",
        "      self.ups.append(DoubleConv(feature*2, feature))  # connect\n",
        "\n",
        "    # adjust the final kernel\n",
        "    self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    skip_connections = []\n",
        "\n",
        "    # down sampling\n",
        "    for down in self.downs:\n",
        "      x = down(x)\n",
        "      skip_connections.append(x)  # save for later skip connection\n",
        "      x = F.max_pool2d(x, (2, 2))  # (kernel_size, stride)\n",
        "\n",
        "    # bottle neck\n",
        "    x = self.bottleneck(x)\n",
        "\n",
        "    # reverse for skip connection\n",
        "    skip_connections.reverse()\n",
        "\n",
        "    # two module as a set in ups, step=2\n",
        "    for i in range(0, len(self.ups), 2):\n",
        "\n",
        "      # self.ups[i] is nn.ConvTranspose2d\n",
        "      x = self.ups[i](x)\n",
        "      skip_connection = skip_connections[i//2]\n",
        "\n",
        "      # transpose + down\n",
        "      concat = torch.cat((skip_connection, x), dim=1)\n",
        "\n",
        "      # self.ups[i] is nn.DoubleConv2d, up-sample\n",
        "      x = self.ups[i+1](concat)\n",
        "\n",
        "    return self.final_conv(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGOOi_ziiPXE",
        "outputId": "aabb8e31-30b1-4b93-87d5-64913b8b40aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 1, 240, 160])\n"
          ]
        }
      ],
      "source": [
        "# Test dimension\n",
        "model = UNet()\n",
        "toy_data = torch.ones(16, 3, 240, 160)  # 3 is RGB\n",
        "out = model(toy_data)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MO5Xg0pUJbNp"
      },
      "outputs": [],
      "source": [
        "model = UNet()\n",
        "model = model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ILgSgfqFJidq"
      },
      "outputs": [],
      "source": [
        "# Load data from Google Drive\n",
        "class CustomDataset(Dataset):  # pytorch Dataset\n",
        "  def __init__(self, image_dir, mask_dir, transform):\n",
        "    super().__init__()\n",
        "    self.image_dir = image_dir\n",
        "    self.mask_dir = mask_dir\n",
        "    self.transform = transform\n",
        "    self.images = os.listdir(self.image_dir)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    image_path = os.path.join(self.image_dir, self.images[i])\n",
        "    mask_path = os.path.join(self.mask_dir, self.images[i].replace('.jpg', '_mask.gif'))\n",
        "    image = np.array(Image.open(image_path))\n",
        "    mask = np.array(Image.open(mask_path).convert('L'))  # L is single channel\n",
        "    return self.transform(image), self.transform(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eqoqZEaHUzU",
        "outputId": "071022b6-e87b-43cf-b8ea-1a91736e8d0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Check the device we are using is GPU or CPU\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sBoa09DRHUtm"
      },
      "outputs": [],
      "source": [
        "# Constants for UNet model training process\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 2\n",
        "IMG_WIDTH = 240\n",
        "IMG_HEIGHT = 160"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rd67NulqHUly"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "all_data = CustomDataset('small_train', 'small_train_masks', T.Compose([T.ToTensor(), T.Resize((IMG_HEIGHT, IMG_WIDTH))]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "GwM6Vz5NKtfm"
      },
      "outputs": [],
      "source": [
        "# Split data into train and val\n",
        "train_data, val_data = torch.utils.data.random_split(all_data, [0.7, 0.3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57mASWwZiaAv",
        "outputId": "4a25174e-b6e8-4859-b497-d10d5833a8a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1600\n",
            "1120\n",
            "480\n"
          ]
        }
      ],
      "source": [
        "print(len(all_data))\n",
        "print(len(train_data))\n",
        "print(len(val_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Gp2ZXGzHLYmt"
      },
      "outputs": [],
      "source": [
        "# Create loader for mini-batch gradient descent\n",
        "from torch.utils.data import DataLoader\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "jEJ-RbO6UzJP"
      },
      "outputs": [],
      "source": [
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fYS30O-dSl0L"
      },
      "outputs": [],
      "source": [
        "def train(model, num_epochs, train_loader, optimizer, print_every=30):\n",
        "  for epoch in range(num_epochs):\n",
        "    for count, (x, y) in enumerate(train_loader):\n",
        "      model.train()\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "      out = model(x)\n",
        "      if count % print_every == 0:\n",
        "        eval(model, val_loader, epoch)\n",
        "      out = torch.sigmoid(out)\n",
        "      loss = loss_function(out, y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4weW5Wi8RWMW"
      },
      "outputs": [],
      "source": [
        "def eval(model, val_loader, epoch):\n",
        "  model.eval()\n",
        "  num_correct = 0\n",
        "  num_pixels = 0\n",
        "  with torch.no_grad():\n",
        "    for x, y in val_loader:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "      out_img = model(x)\n",
        "      probability = torch.sigmoid(out_img)\n",
        "      predictions = probability>0.5\n",
        "      num_correct += (predictions==y).sum()\n",
        "      num_pixels += BATCH_SIZE*IMG_WIDTH*IMG_HEIGHT\n",
        "  print(f'Epoch[{epoch+1}] Acc: {num_correct/num_pixels}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rcL1usEWTHq",
        "outputId": "5c9b0262-75f9-49bd-b9bb-8c7f95a91353"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch[1] Acc: 0.7845152020454407\n",
            "Epoch[1] Acc: 0.9618967175483704\n",
            "Epoch[1] Acc: 0.9401195645332336\n",
            "Epoch[2] Acc: 0.9822676181793213\n",
            "Epoch[2] Acc: 0.9828576445579529\n",
            "Epoch[2] Acc: 0.9641614556312561\n"
          ]
        }
      ],
      "source": [
        "train(model, NUM_EPOCHS, train_loader, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLmSx5eZGq8t"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}